{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "artificial-discretion",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Working with Open Model Zoo Models\n",
    "This tutorial shows how to download a model from the [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo), convert it to OpenVINO's IR format, show information about the model, and benchmark the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dda33e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## OpenVINO and Open Model Zoo Tools\n",
    "\n",
    "The OpenVINO and Open Model Zoo tools are listed in the table below.\n",
    "\n",
    "| Tool             | Command             | Description                                             |\n",
    "|:-----------------|:--------------------|:--------------------------------------------------------|\n",
    "| Model Downloader | omz_downloader      | Download models from Open Model Zoo                     |\n",
    "| Model Converter  | omz_converter       | Convert Open Model Zoo models to OpenVINO's IR format   |\n",
    "| Info Dumper      | omz_info_dumper     | Print information about Open Model Zoo models           |\n",
    "| Benchmark Tool   | benchmark_app       | Benchmark model performance by computing inference time |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-preview",
   "metadata": {},
   "source": [
    "## Preperation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c2dfff-d594-42b3-94fd-70b9726b48ea",
   "metadata": {},
   "source": [
    "### Install OpenVINO dev-tools in order to use them on your local machine\n",
    "### **NOTE: Below command is not required if you are on Devcloud**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ef9f3b7-aea9-4ea2-a3d2-1a137f5f51e7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openvino-dev[caffe,kaldi,mxnet,onnx,pytorch,tensorflow2]==2022.1.0\n",
      "  Using cached openvino_dev-2022.1.0-7019-py3-none-any.whl (5.8 MB)\n",
      "Collecting numpy<=1.21,>=1.16.6\n",
      "  Downloading numpy-1.21.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.7 MB 7.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pillow>=8.1.2\n",
      "  Using cached Pillow-9.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Collecting jstyleson~=0.0.2\n",
      "  Using cached jstyleson-0.0.2-py3-none-any.whl\n",
      "Collecting pandas~=1.1.5\n",
      "  Using cached pandas-1.1.5-cp38-cp38-manylinux1_x86_64.whl (9.3 MB)\n",
      "Collecting tqdm>=4.54.1\n",
      "  Using cached tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
      "Collecting addict>=2.4.0\n",
      "  Using cached addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
      "Collecting scikit-image>=0.17.2\n",
      "  Using cached scikit_image-0.19.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
      "Collecting nibabel>=3.2.1\n",
      "  Using cached nibabel-3.2.2-py3-none-any.whl (3.3 MB)\n",
      "Collecting fast-ctc-decode>=0.2.5\n",
      "  Using cached fast_ctc_decode-0.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (517 kB)\n",
      "Collecting shapely>=1.7.1\n",
      "  Using cached Shapely-1.8.2-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
      "Collecting pyclipper>=1.2.1\n",
      "  Using cached pyclipper-1.3.0.post2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (617 kB)\n",
      "Collecting transformers>=4.5\n",
      "  Using cached transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
      "Collecting openvino-telemetry>=2022.1.0\n",
      "  Using cached openvino_telemetry-2022.1.1-py3-none-any.whl (20 kB)\n",
      "Collecting pyyaml>=5.4.1\n",
      "  Using cached PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\n",
      "Collecting scikit-learn~=0.24.1\n",
      "  Using cached scikit_learn-0.24.2-cp38-cp38-manylinux2010_x86_64.whl (24.9 MB)\n",
      "Collecting texttable~=1.6.3\n",
      "  Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB)\n",
      "Collecting rawpy>=0.16.0\n",
      "  Using cached rawpy-0.17.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
      "Collecting opencv-python==4.5.*\n",
      "  Using cached opencv_python-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.5 MB)\n",
      "Collecting scipy~=1.5.4\n",
      "  Using cached scipy-1.5.4-cp38-cp38-manylinux1_x86_64.whl (25.8 MB)\n",
      "Collecting parasail>=1.2.4\n",
      "  Using cached parasail-1.2.4-py2.py3-none-manylinux2010_x86_64.whl (14.1 MB)\n",
      "Collecting py-cpuinfo>=7.0.0\n",
      "  Using cached py_cpuinfo-8.0.0-py3-none-any.whl\n",
      "Collecting numpy<=1.21,>=1.16.6\n",
      "  Downloading numpy-1.19.5-cp38-cp38-manylinux2010_x86_64.whl (14.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.9 MB 40.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting networkx~=2.6\n",
      "  Using cached networkx-2.8.2-py3-none-any.whl (2.0 MB)\n",
      "Collecting pydicom>=2.1.2\n",
      "  Using cached pydicom-2.3.0-py3-none-any.whl (2.0 MB)\n",
      "Collecting lmdb>=1.2.1\n",
      "  Using cached lmdb-1.3.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (305 kB)\n",
      "Requirement already satisfied: requests>=2.25.1 in /home/intel/anaconda3/envs/wksp/lib/python3.8/site-packages (from openvino-dev[caffe,kaldi,mxnet,onnx,pytorch,tensorflow2]==2022.1.0) (2.27.1)\n",
      "Collecting openvino==2022.1.0\n",
      "  Using cached openvino-2022.1.0-7019-cp38-cp38-manylinux_2_27_x86_64.whl (26.1 MB)\n",
      "Collecting imagecodecs\n",
      "  Using cached imagecodecs-2022.2.22-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.2 MB)\n",
      "Collecting sentencepiece>=0.1.95\n",
      "  Using cached sentencepiece-0.1.96-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Collecting progress>=1.5\n",
      "  Using cached progress-1.6-py3-none-any.whl\n",
      "Collecting nltk>=3.5\n",
      "  Using cached nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "Collecting tokenizers~=0.10.1\n",
      "  Using cached tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in /home/intel/anaconda3/envs/wksp/lib/python3.8/site-packages (from openvino-dev[caffe,kaldi,mxnet,onnx,pytorch,tensorflow2]==2022.1.0) (0.7.1)\n",
      "Requirement already satisfied: fastjsonschema~=2.15.1 in /home/intel/anaconda3/envs/wksp/lib/python3.8/site-packages (from openvino-dev[caffe,kaldi,mxnet,onnx,pytorch,tensorflow2]==2022.1.0) (2.15.3)\n",
      "Collecting mxnet~=1.7.0.post2\n",
      "  Using cached mxnet-1.7.0.post2-py2.py3-none-manylinux2014_x86_64.whl (54.7 MB)\n",
      "Requirement already satisfied: urllib3>=1.26.4 in /home/intel/anaconda3/envs/wksp/lib/python3.8/site-packages (from openvino-dev[caffe,kaldi,mxnet,onnx,pytorch,tensorflow2]==2022.1.0) (1.26.9)\n",
      "Collecting protobuf>=3.15.6\n",
      "  Using cached protobuf-4.21.1-cp37-abi3-manylinux2014_x86_64.whl (407 kB)\n",
      "Collecting onnx>=1.8.1\n",
      "  Using cached onnx-1.11.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (12.8 MB)\n",
      "Collecting torchvision==0.9.1\n",
      "  Using cached torchvision-0.9.1-cp38-cp38-manylinux1_x86_64.whl (17.4 MB)\n",
      "Collecting torch==1.8.1\n",
      "  Using cached torch-1.8.1-cp38-cp38-manylinux1_x86_64.whl (804.1 MB)\n",
      "Collecting yacs>=0.1.8\n",
      "  Using cached yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Collecting tensorflow~=2.5.3\n",
      "  Using cached tensorflow-2.5.3-cp38-cp38-manylinux2010_x86_64.whl (460.4 MB)\n",
      "Collecting typing-extensions\n",
      "  Using cached typing_extensions-4.2.0-py3-none-any.whl (24 kB)\n",
      "Collecting graphviz<0.9.0,>=0.8.1\n",
      "  Using cached graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: packaging>=14.3 in /home/intel/anaconda3/envs/wksp/lib/python3.8/site-packages (from nibabel>=3.2.1->openvino-dev[caffe,kaldi,mxnet,onnx,pytorch,tensorflow2]==2022.1.0) (21.3)\n",
      "Requirement already satisfied: setuptools in /home/intel/anaconda3/envs/wksp/lib/python3.8/site-packages (from nibabel>=3.2.1->openvino-dev[caffe,kaldi,mxnet,onnx,pytorch,tensorflow2]==2022.1.0) (61.2.0)\n",
      "Collecting click\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "Collecting regex>=2021.8.3\n",
      "  Using cached regex-2022.4.24-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (764 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/intel/anaconda3/envs/wksp/lib/python3.8/site-packages (from packaging>=14.3->nibabel>=3.2.1->openvino-dev[caffe,kaldi,mxnet,onnx,pytorch,tensorflow2]==2022.1.0) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/intel/anaconda3/envs/wksp/lib/python3.8/site-packages (from pandas~=1.1.5->openvino-dev[caffe,kaldi,mxnet,onnx,pytorch,tensorflow2]==2022.1.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/intel/anaconda3/envs/wksp/lib/python3.8/site-packages (from pandas~=1.1.5->openvino-dev[caffe,kaldi,mxnet,onnx,pytorch,tensorflow2]==2022.1.0) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/intel/anaconda3/envs/wksp/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas~=1.1.5->openvino-dev[caffe,kaldi,mxnet,onnx,pytorch,tensorflow2]==2022.1.0) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/intel/anaconda3/envs/wksp/lib/python3.8/site-packages (from requests>=2.25.1->openvino-dev[caffe,kaldi,mxnet,onnx,pytorch,tensorflow2]==2022.1.0) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/intel/anaconda3/envs/wksp/lib/python3.8/site-packages (from requests>=2.25.1->openvino-dev[caffe,kaldi,mxnet,onnx,pytorch,tensorflow2]==2022.1.0) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/intel/anaconda3/envs/wksp/lib/python3.8/site-packages (from requests>=2.25.1->openvino-dev[caffe,kaldi,mxnet,onnx,pytorch,tensorflow2]==2022.1.0) (2.0.12)\n",
      "Collecting tifffile>=2019.7.26\n",
      "  Using cached tifffile-2022.5.4-py3-none-any.whl (195 kB)\n",
      "Collecting PyWavelets>=1.1.1\n",
      "  Using cached PyWavelets-1.3.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.9 MB)\n",
      "Collecting imageio>=2.4.1\n",
      "  Using cached imageio-2.19.3-py3-none-any.whl (3.4 MB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: wheel~=0.35 in /home/intel/anaconda3/envs/wksp/lib/python3.8/site-packages (from tensorflow~=2.5.3->openvino-dev[caffe,kaldi,mxnet,onnx,pytorch,tensorflow2]==2022.1.0) (0.37.1)\n",
      "Collecting typing-extensions\n",
      "  Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting keras-nightly~=2.5.0.dev\n",
      "  Using cached keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n",
      "Collecting google-pasta~=0.2\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting grpcio~=1.34.0\n",
      "  Using cached grpcio-1.34.1-cp38-cp38-manylinux2014_x86_64.whl (4.0 MB)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Using cached termcolor-1.1.0-py3-none-any.whl\n",
      "Collecting gast==0.4.0\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting tensorflow-estimator<2.6.0,>=2.5.0\n",
      "  Using cached tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
      "Collecting absl-py~=0.10\n",
      "  Using cached absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
      "Collecting h5py~=3.1.0\n",
      "  Using cached h5py-3.1.0-cp38-cp38-manylinux1_x86_64.whl (4.4 MB)\n",
      "Collecting tensorboard~=2.5\n",
      "  Using cached tensorboard-2.9.0-py3-none-any.whl (5.8 MB)\n",
      "Collecting six>=1.5\n",
      "  Using cached six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting wrapt~=1.12.1\n",
      "  Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.7-py3-none-any.whl (97 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.6.6-py2.py3-none-any.whl (156 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Using cached Werkzeug-2.1.2-py3-none-any.whl (224 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/intel/anaconda3/envs/wksp/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow~=2.5.3->openvino-dev[caffe,kaldi,mxnet,onnx,pytorch,tensorflow2]==2022.1.0) (4.11.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/intel/anaconda3/envs/wksp/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.5->tensorflow~=2.5.3->openvino-dev[caffe,kaldi,mxnet,onnx,pytorch,tensorflow2]==2022.1.0) (3.8.0)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.7.0-py3-none-any.whl (10 kB)\n",
      "Collecting transformers>=4.5\n",
      "  Using cached transformers-4.19.1-py3-none-any.whl (4.2 MB)\n",
      "  Using cached transformers-4.19.0-py3-none-any.whl (4.2 MB)\n",
      "  Using cached transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.53-py3-none-any.whl\n",
      "Collecting transformers>=4.5\n",
      "  Using cached transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
      "  Using cached transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Using cached huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
      "Installing collected packages: pyasn1, six, rsa, pyasn1-modules, oauthlib, cachetools, typing-extensions, tqdm, requests-oauthlib, regex, pyyaml, pillow, numpy, joblib, google-auth, filelock, click, werkzeug, tokenizers, tifffile, threadpoolctl, tensorboard-plugin-wit, tensorboard-data-server, scipy, sacremoses, PyWavelets, protobuf, networkx, markdown, imageio, huggingface-hub, grpcio, google-auth-oauthlib, absl-py, wrapt, transformers, torch, texttable, termcolor, tensorflow-estimator, tensorboard, shapely, sentencepiece, scikit-learn, scikit-image, rawpy, pydicom, pyclipper, py-cpuinfo, progress, parasail, pandas, opt-einsum, openvino-telemetry, openvino, opencv-python, nltk, nibabel, lmdb, keras-preprocessing, keras-nightly, jstyleson, imagecodecs, h5py, graphviz, google-pasta, gast, flatbuffers, fast-ctc-decode, astunparse, addict, yacs, torchvision, tensorflow, openvino-dev, onnx, mxnet\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.22.4\n",
      "    Uninstalling numpy-1.22.4:\n",
      "      Successfully uninstalled numpy-1.22.4\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.4.2\n",
      "    Uninstalling pandas-1.4.2:\n",
      "      Successfully uninstalled pandas-1.4.2\n",
      "Successfully installed PyWavelets-1.3.0 absl-py-0.15.0 addict-2.4.0 astunparse-1.6.3 cachetools-5.2.0 click-8.1.3 fast-ctc-decode-0.3.2 filelock-3.7.0 flatbuffers-1.12 gast-0.4.0 google-auth-2.6.6 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 graphviz-0.8.4 grpcio-1.34.1 h5py-3.1.0 huggingface-hub-0.7.0 imagecodecs-2022.2.22 imageio-2.19.3 joblib-1.1.0 jstyleson-0.0.2 keras-nightly-2.5.0.dev2021032900 keras-preprocessing-1.1.2 lmdb-1.3.0 markdown-3.3.7 mxnet-1.7.0.post2 networkx-2.8.2 nibabel-3.2.2 nltk-3.7 numpy-1.19.5 oauthlib-3.2.0 onnx-1.11.0 opencv-python-4.5.5.64 openvino-2022.1.0 openvino-dev-2022.1.0 openvino-telemetry-2022.1.1 opt-einsum-3.3.0 pandas-1.1.5 parasail-1.2.4 pillow-9.1.1 progress-1.6 protobuf-4.21.1 py-cpuinfo-8.0.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pyclipper-1.3.0.post2 pydicom-2.3.0 pyyaml-6.0 rawpy-0.17.1 regex-2022.4.24 requests-oauthlib-1.3.1 rsa-4.8 sacremoses-0.0.53 scikit-image-0.19.2 scikit-learn-0.24.2 scipy-1.5.4 sentencepiece-0.1.96 shapely-1.8.2 six-1.15.0 tensorboard-2.9.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.5.3 tensorflow-estimator-2.5.0 termcolor-1.1.0 texttable-1.6.4 threadpoolctl-3.1.0 tifffile-2022.5.4 tokenizers-0.10.3 torch-1.8.1 torchvision-0.9.1 tqdm-4.64.0 transformers-4.16.2 typing-extensions-3.7.4.3 werkzeug-2.1.2 wrapt-1.12.1 yacs-0.1.8\n"
     ]
    }
   ],
   "source": [
    "## Use below command to download openvino latest version with devtools \n",
    "!pip install openvino-dev[onnx,pytorch,mxnet,kaldi,caffe,tensorflow2]==2022.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665320ef-01ad-457b-a153-92ba5af98269",
   "metadata": {},
   "source": [
    "## Download Model from Open Model Zoo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rising-interval",
   "metadata": {},
   "source": [
    "Specify, display and run the Model Downloader command to download the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df2b0446-0c49-41cf-9f3f-dec1232249f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: omz_downloader [-h] [--name PAT[,PAT...]] [--list FILE.LST] [--all]\n",
      "                      [--print_all] [--precisions PREC[,PREC...]] [-o DIR]\n",
      "                      [--cache_dir DIR] [--num_attempts N]\n",
      "                      [--progress_format {text,json}] [-j N]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --name PAT[,PAT...]   download only models whose names match at least one of\n",
      "                        the specified patterns\n",
      "  --list FILE.LST       download only models whose names match at least one of\n",
      "                        the patterns in the specified file\n",
      "  --all                 download all available models\n",
      "  --print_all           print all available models\n",
      "  --precisions PREC[,PREC...]\n",
      "                        download only models with the specified precisions\n",
      "                        (actual for DLDT networks); specify one or more of:\n",
      "                        FP16,FP16-INT1,FP16-INT8,FP32,FP32-INT1,FP32-INT8\n",
      "  -o DIR, --output_dir DIR\n",
      "                        path where to save models\n",
      "  --cache_dir DIR       directory to use as a cache for downloaded files\n",
      "  --num_attempts N      attempt each download up to N times\n",
      "  --progress_format {text,json}\n",
      "                        which format to use for progress reporting\n",
      "  -j N, --jobs N        how many downloads to perform concurrently\n"
     ]
    }
   ],
   "source": [
    "## Uncomment the next line to show omz_downloader's help which explains the command line options\n",
    "!omz_downloader -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "556d0c12-15cf-492d-a1ed-41dff5090eff",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sphereface\n",
      "aclnet\n",
      "aclnet-int8\n",
      "action-recognition-0001\n",
      "age-gender-recognition-retail-0013\n",
      "alexnet\n",
      "anti-spoof-mn3\n",
      "asl-recognition-0004\n",
      "background-matting-mobilenetv2\n",
      "bert-base-ner\n",
      "bert-large-uncased-whole-word-masking-squad-0001\n",
      "bert-large-uncased-whole-word-masking-squad-emb-0001\n",
      "bert-large-uncased-whole-word-masking-squad-int8-0001\n",
      "bert-small-uncased-whole-word-masking-squad-0001\n",
      "bert-small-uncased-whole-word-masking-squad-0002\n",
      "bert-small-uncased-whole-word-masking-squad-emb-int8-0001\n",
      "bert-small-uncased-whole-word-masking-squad-int8-0002\n",
      "brain-tumor-segmentation-0001\n",
      "brain-tumor-segmentation-0002\n",
      "caffenet\n",
      "cocosnet\n",
      "colorization-siggraph\n",
      "colorization-v2\n",
      "common-sign-language-0001\n",
      "common-sign-language-0002\n",
      "ctdet_coco_dlav0_512\n",
      "ctpn\n",
      "deblurgan-v2\n",
      "deeplabv3\n",
      "densenet-121\n",
      "densenet-121-tf\n",
      "detr-resnet50\n",
      "dla-34\n",
      "driver-action-recognition-adas-0002\n",
      "drn-d-38\n",
      "efficientdet-d0-tf\n",
      "efficientdet-d1-tf\n",
      "efficientnet-b0\n",
      "efficientnet-b0-pytorch\n",
      "efficientnet-v2-b0\n",
      "efficientnet-v2-s\n",
      "emotions-recognition-retail-0003\n",
      "f3net\n",
      "face-detection-0200\n",
      "face-detection-0202\n",
      "face-detection-0204\n",
      "face-detection-0205\n",
      "face-detection-0206\n",
      "face-detection-adas-0001\n",
      "face-detection-retail-0004\n",
      "face-detection-retail-0005\n",
      "face-detection-retail-0044\n",
      "face-recognition-resnet100-arcface-onnx\n",
      "face-reidentification-retail-0095\n",
      "faceboxes-pytorch\n",
      "facenet-20180408-102900\n",
      "facial-landmarks-35-adas-0002\n",
      "facial-landmarks-98-detection-0001\n",
      "fast-neural-style-mosaic-onnx\n",
      "faster-rcnn-resnet101-coco-sparse-60-0001\n",
      "faster_rcnn_inception_resnet_v2_atrous_coco\n",
      "faster_rcnn_resnet50_coco\n",
      "fastseg-large\n",
      "fastseg-small\n",
      "fbcnn\n",
      "fcrn-dp-nyu-depth-v2-tf\n",
      "formula-recognition-medium-scan-0001\n",
      "formula-recognition-polynomials-handwritten-0001\n",
      "forward-tacotron\n",
      "gaze-estimation-adas-0002\n",
      "gmcnn-places2-tf\n",
      "googlenet-v1\n",
      "googlenet-v1-tf\n",
      "googlenet-v2\n",
      "googlenet-v2-tf\n",
      "googlenet-v3\n",
      "googlenet-v3-pytorch\n",
      "googlenet-v4-tf\n",
      "gpt-2\n",
      "handwritten-english-recognition-0001\n",
      "handwritten-japanese-recognition-0001\n",
      "handwritten-score-recognition-0003\n",
      "handwritten-simplified-chinese-recognition-0001\n",
      "hbonet-0.25\n",
      "hbonet-1.0\n",
      "head-pose-estimation-adas-0001\n",
      "higher-hrnet-w32-human-pose-estimation\n",
      "horizontal-text-detection-0001\n",
      "hrnet-v2-c1-segmentation\n",
      "human-pose-estimation-0001\n",
      "human-pose-estimation-0005\n",
      "human-pose-estimation-0006\n",
      "human-pose-estimation-0007\n",
      "human-pose-estimation-3d-0001\n",
      "hybrid-cs-model-mri\n",
      "i3d-rgb-tf\n",
      "icnet-camvid-ava-0001\n",
      "icnet-camvid-ava-sparse-30-0001\n",
      "icnet-camvid-ava-sparse-60-0001\n",
      "image-retrieval-0001\n",
      "inception-resnet-v2-tf\n",
      "instance-segmentation-person-0007\n",
      "instance-segmentation-security-0002\n",
      "instance-segmentation-security-0091\n",
      "instance-segmentation-security-0228\n",
      "instance-segmentation-security-1039\n",
      "instance-segmentation-security-1040\n",
      "landmarks-regression-retail-0009\n",
      "license-plate-recognition-barrier-0001\n",
      "license-plate-recognition-barrier-0007\n",
      "machine-translation-nar-de-en-0002\n",
      "machine-translation-nar-en-de-0002\n",
      "machine-translation-nar-en-ru-0002\n",
      "machine-translation-nar-ru-en-0002\n",
      "mask_rcnn_inception_resnet_v2_atrous_coco\n",
      "mask_rcnn_resnet50_atrous_coco\n",
      "midasnet\n",
      "mixnet-l\n",
      "mobilefacedet-v1-mxnet\n",
      "mobilenet-ssd\n",
      "mobilenet-v1-0.25-128\n",
      "mobilenet-v1-1.0-224\n",
      "mobilenet-v1-1.0-224-tf\n",
      "mobilenet-v2\n",
      "mobilenet-v2-1.0-224\n",
      "mobilenet-v2-1.4-224\n",
      "mobilenet-v2-pytorch\n",
      "mobilenet-v3-large-1.0-224-tf\n",
      "mobilenet-v3-small-1.0-224-tf\n",
      "mobilenet-yolo-v4-syg\n",
      "mozilla-deepspeech-0.6.1\n",
      "mozilla-deepspeech-0.8.2\n",
      "mtcnn\n",
      "netvlad-tf\n",
      "nfnet-f0\n",
      "noise-suppression-denseunet-ll-0001\n",
      "noise-suppression-poconetlike-0001\n",
      "ocrnet-hrnet-w48-paddle\n",
      "octave-resnet-26-0.25\n",
      "open-closed-eye-0001\n",
      "pedestrian-and-vehicle-detector-adas-0001\n",
      "pedestrian-detection-adas-0002\n",
      "pelee-coco\n",
      "person-attributes-recognition-crossroad-0230\n",
      "person-attributes-recognition-crossroad-0234\n",
      "person-attributes-recognition-crossroad-0238\n",
      "person-detection-0106\n",
      "person-detection-0200\n",
      "person-detection-0201\n",
      "person-detection-0202\n",
      "person-detection-0203\n",
      "person-detection-0301\n",
      "person-detection-0302\n",
      "person-detection-0303\n",
      "person-detection-action-recognition-0005\n",
      "person-detection-action-recognition-0006\n",
      "person-detection-action-recognition-teacher-0002\n",
      "person-detection-asl-0001\n",
      "person-detection-raisinghand-recognition-0001\n",
      "person-detection-retail-0002\n",
      "person-detection-retail-0013\n",
      "person-reidentification-retail-0277\n",
      "person-reidentification-retail-0286\n",
      "person-reidentification-retail-0287\n",
      "person-reidentification-retail-0288\n",
      "person-vehicle-bike-detection-2000\n",
      "person-vehicle-bike-detection-2001\n",
      "person-vehicle-bike-detection-2002\n",
      "person-vehicle-bike-detection-2003\n",
      "person-vehicle-bike-detection-2004\n",
      "person-vehicle-bike-detection-crossroad-0078\n",
      "person-vehicle-bike-detection-crossroad-1016\n",
      "person-vehicle-bike-detection-crossroad-yolov3-1020\n",
      "product-detection-0001\n",
      "pspnet-pytorch\n",
      "quartznet-15x5-en\n",
      "regnetx-3.2gf\n",
      "repvgg-a0\n",
      "repvgg-b1\n",
      "repvgg-b3\n",
      "resnest-50-pytorch\n",
      "resnet-18-pytorch\n",
      "resnet-34-pytorch\n",
      "resnet-50-pytorch\n",
      "resnet-50-tf\n",
      "resnet18-xnor-binary-onnx-0001\n",
      "resnet50-binary-0001\n",
      "retinaface-resnet50-pytorch\n",
      "retinanet-tf\n",
      "rexnet-v1-x1.0\n",
      "rfcn-resnet101-coco-tf\n",
      "road-segmentation-adas-0001\n",
      "robust-video-matting-mobilenetv3\n",
      "se-inception\n",
      "se-resnet-50\n",
      "se-resnext-50\n",
      "semantic-segmentation-adas-0001\n",
      "shufflenet-v2-x0.5\n",
      "shufflenet-v2-x1.0\n",
      "single-human-pose-estimation-0001\n",
      "single-image-super-resolution-1032\n",
      "single-image-super-resolution-1033\n",
      "smartlab-object-detection-0001\n",
      "smartlab-object-detection-0002\n",
      "smartlab-object-detection-0003\n",
      "smartlab-object-detection-0004\n",
      "smartlab-sequence-modelling-0001\n",
      "squeezenet1.0\n",
      "squeezenet1.1\n",
      "ssd-resnet34-1200-onnx\n",
      "ssd300\n",
      "ssd512\n",
      "ssd_mobilenet_v1_coco\n",
      "ssd_mobilenet_v1_fpn_coco\n",
      "ssdlite_mobilenet_v2\n",
      "swin-tiny-patch4-window7-224\n",
      "t2t-vit-14\n",
      "text-detection-0003\n",
      "text-detection-0004\n",
      "text-image-super-resolution-0001\n",
      "text-recognition-0012\n",
      "text-recognition-0014\n",
      "text-recognition-0015\n",
      "text-recognition-0016\n",
      "text-recognition-resnet-fc\n",
      "text-spotting-0005\n",
      "text-to-speech-en-0001\n",
      "text-to-speech-en-multi-0001\n",
      "time-series-forecasting-electricity-0001\n",
      "ultra-lightweight-face-detection-rfb-320\n",
      "ultra-lightweight-face-detection-slim-320\n",
      "unet-camvid-onnx-0001\n",
      "vehicle-attributes-recognition-barrier-0039\n",
      "vehicle-attributes-recognition-barrier-0042\n",
      "vehicle-detection-0200\n",
      "vehicle-detection-0201\n",
      "vehicle-detection-0202\n",
      "vehicle-detection-adas-0002\n",
      "vehicle-license-plate-detection-barrier-0106\n",
      "vehicle-license-plate-detection-barrier-0123\n",
      "vehicle-reid-0001\n",
      "vgg16\n",
      "vgg19\n",
      "vitstr-small-patch16-224\n",
      "wav2vec2-base\n",
      "wavernn\n",
      "weld-porosity-detection-0001\n",
      "yolact-resnet50-fpn-pytorch\n",
      "yolo-v1-tiny-tf\n",
      "yolo-v2-ava-0001\n",
      "yolo-v2-ava-sparse-35-0001\n",
      "yolo-v2-ava-sparse-70-0001\n",
      "yolo-v2-tf\n",
      "yolo-v2-tiny-ava-0001\n",
      "yolo-v2-tiny-ava-sparse-30-0001\n",
      "yolo-v2-tiny-ava-sparse-60-0001\n",
      "yolo-v2-tiny-tf\n",
      "yolo-v2-tiny-vehicle-detection-0001\n",
      "yolo-v3-onnx\n",
      "yolo-v3-tf\n",
      "yolo-v3-tiny-onnx\n",
      "yolo-v3-tiny-tf\n",
      "yolo-v4-tf\n",
      "yolo-v4-tiny-tf\n",
      "yolof\n",
      "yolox-tiny\n"
     ]
    }
   ],
   "source": [
    "!omz_downloader --print_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-checklist",
   "metadata": {},
   "source": [
    "## Convert Model to OpenVINO IR format\n",
    "\n",
    "Specify, display and run the Model Converter command to convert the model to IR format. Model Conversion may take a while. The output of the Model Converter command will be displayed. Conversion succeeded if the last lines of the output include `[ SUCCESS ] Generated IR version 11 model.` For downloaded models that are already in IR format, conversion will be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11fe7461-90db-4585-b55f-b3df42b01274",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: omz_converter [-h] [-d DIR] [-o DIR] [--name PAT[,PAT...]]\n",
      "                     [--list FILE.LST] [--all] [--print_all]\n",
      "                     [--precisions PREC[,PREC...]] [-p PYTHON] [--mo MO.PY]\n",
      "                     [--add_mo_arg ARG] [--dry_run] [-j JOBS]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -d DIR, --download_dir DIR\n",
      "                        root of the directory tree with downloaded model files\n",
      "  -o DIR, --output_dir DIR\n",
      "                        root of the directory tree to place converted files\n",
      "                        into\n",
      "  --name PAT[,PAT...]   convert only models whose names match at least one of\n",
      "                        the specified patterns\n",
      "  --list FILE.LST       convert only models whose names match at least one of\n",
      "                        the patterns in the specified file\n",
      "  --all                 convert all available models\n",
      "  --print_all           print all available models\n",
      "  --precisions PREC[,PREC...]\n",
      "                        run only conversions that produce models with the\n",
      "                        specified precisions\n",
      "  -p PYTHON, --python PYTHON\n",
      "                        Python executable to run Model Optimizer with\n",
      "  --mo MO.PY            Model Optimizer entry point script\n",
      "  --add_mo_arg ARG      Extra argument to pass to Model Optimizer\n",
      "  --dry_run             Print the conversion commands without running them\n",
      "  -j JOBS, --jobs JOBS  number of conversions to run concurrently\n"
     ]
    }
   ],
   "source": [
    "## Uncomment the next line to show omz_converter's help which explains the command line options\n",
    "!omz_converter --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "engaged-academy",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: omz_conv: command not found\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa8d655f-215d-4e3c-adcb-e8fd4a2e8ab4",
   "metadata": {},
   "source": [
    "## Get Model Information\n",
    "\n",
    "The Info Dumper prints the following information for Open Model Zoo models:\n",
    "\n",
    "* Model name\n",
    "* Description\n",
    "* Framework that was used to train the model\n",
    "* License URL\n",
    "* Precisions supported by the model\n",
    "* Subdirectory: the location of the downloaded model\n",
    "* Task type\n",
    "\n",
    "This information can be shown by running `omz_info_dumper --name model_name` in a terminal. The information can also be parsed and used in scripts. \n",
    "\n",
    "In the next cell, we run Info Dumper and use json to load the information in a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8247daf-d3c5-4420-b4c8-d305ac4ace5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: omz_info_dumper [-h] [--name PAT[,PAT...]] [--list FILE.LST] [--all]\n",
      "                       [--print_all]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help           show this help message and exit\n",
      "  --name PAT[,PAT...]  only dump info for models whose names match at least\n",
      "                       one of the specified patterns\n",
      "  --list FILE.LST      only dump info for models whose names match at least\n",
      "                       one of the patterns in the specified file\n",
      "  --all                dump info for all available models\n",
      "  --print_all          print all available models\n"
     ]
    }
   ],
   "source": [
    "!omz_info_dumper -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea7e868-fd2d-4d11-9c87-7aa1f1301083",
   "metadata": {},
   "source": [
    "Having the model information in a JSON file allows us to extract the path to the model directory, and build the path to the IR file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e01154-f700-479f-9111-147c95595d46",
   "metadata": {},
   "source": [
    "## Run Benchmark Tool\n",
    "\n",
    "By default, Benchmark Tool runs inference for 60 seconds in asynchronous mode on CPU. It returns inference speed as latency (milliseconds per image) and throughput (frames per second) values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282452e8-24c7-49c0-bdb2-10677971c30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment the next line to show benchmark_app's help which explains the command line options\n",
    "!benchmark_app --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75891996-cf53-4c76-ad3c-5fb468ccd7bb",
   "metadata": {},
   "source": [
    "### Benchmark with Different Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d64dd7-789d-4536-ab8f-84999c73afaf",
   "metadata": {},
   "source": [
    "`benchmark_app` displays logging information that is not always necessary. We parse the output with json and show a more compact result\n",
    "\n",
    "The following cells show some examples of `benchmark_app` with different parameters. Some useful parameters are:\n",
    "\n",
    "- `-d` Device to use for inference. For example: CPU, GPU, MULTI. Default: CPU\n",
    "- `-t` Time in number of seconds to run inference. Default: 60\n",
    "- `-api` Use asynchronous (async) or synchronous (sync) inference. Default: async\n",
    "- `-b` Batch size. Default: 1\n",
    "\n",
    "\n",
    "Run `! benchmark_app --help` to get an overview of all possible command line parameters.\n",
    "\n",
    "In the next cell, we define a `benchmark_model()` function that calls `benchmark_app`. This makes it easy to try different combinations. In the cell below that, we display the available devices on the system.\n",
    "\n",
    "> **NOTE**: In this notebook we run benchmark_app for 15 seconds to give a quick indication of performance. For more accurate performance, we recommended running inference for at least one minute by setting the `t` parameter to 60 or higher, and running `benchmark_app` in a terminal/command prompt after closing other applications. You can copy the _benchmark command_ and paste it in a command prompt where you have activated the `openvino_env` environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7742390e-df71-45e1-9572-f3cbaa576ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_model(model_xml, device=\"CPU\", seconds=60, api=\"async\", batch=1):\n",
    "    ie = Core()\n",
    "    model_path = Path(model_xml)\n",
    "    if (\"GPU\" in device) and (\"GPU\" not in ie.available_devices):\n",
    "        DeviceNotFoundAlert(\"GPU\")\n",
    "    else:\n",
    "        benchmark_command = f\"benchmark_app -m {model_path} -d {device} -t {seconds} -api {api} -b {batch}\"\n",
    "        display(Markdown(f\"**Benchmark {model_path.name} with {device} for {seconds} seconds with {api} inference**\"))\n",
    "        display(Markdown(f\"Benchmark command: `{benchmark_command}`\"))\n",
    "\n",
    "        benchmark_output = %sx $benchmark_command\n",
    "        print(\"command ended\")\n",
    "        benchmark_result = [line for line in benchmark_output\n",
    "                            if not (line.startswith(r\"[\") or line.startswith(\"  \") or line == \"\")]\n",
    "        print(\"\\n\".join(benchmark_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298904f0-638c-4958-876a-3b8c8bd06518",
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = Core()\n",
    "\n",
    "# Show devices available for OpenVINO Inference Engine\n",
    "for device in ie.available_devices:\n",
    "    device_name = ie.get_property(device, \"FULL_DEVICE_NAME\")\n",
    "    print(f\"{device}: {device_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486919e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_model(model_path, device=\"CPU\", seconds=15, api=\"async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d4c745-a4c2-4242-a2e7-fff1eb098b98",
   "metadata": {
    "tags": [],
    "test_replace": {
     "seconds=15": "seconds=3"
    }
   },
   "outputs": [],
   "source": [
    "benchmark_model(model_path, device=\"AUTO\", seconds=15, api=\"async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d47e1cf-cb2f-4138-928c-b56891d5c318",
   "metadata": {
    "tags": [],
    "test_replace": {
     "seconds=15": "seconds=3"
    }
   },
   "outputs": [],
   "source": [
    "benchmark_model(model_path, device=\"GPU\", seconds=15, api=\"async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1b0bb2-87c7-44c9-92bf-1d95aa1500a1",
   "metadata": {
    "tags": [],
    "test_replace": {
     "seconds=15": "seconds=3"
    }
   },
   "outputs": [],
   "source": [
    "benchmark_model(model_path, device=\"MULTI:CPU,GPU\", seconds=15, api=\"async\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bc9b317d09174d2c31f34f506ea6004a876573d0732009f97d44018ebb6e02f6"
  },
  "kernelspec": {
   "display_name": "Python [conda env:wksp] *",
   "language": "python",
   "name": "conda-env-wksp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
